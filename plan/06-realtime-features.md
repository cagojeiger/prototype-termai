# 06. ì‹¤ì‹œê°„ ê¸°ëŠ¥ êµ¬í˜„

## ëª©í‘œ
í„°ë¯¸ë„ ì‘ì—…ì„ ë°©í•´í•˜ì§€ ì•Šìœ¼ë©´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ AI ë¶„ì„ê³¼ ë„ì›€ì„ ì œê³µí•˜ëŠ” ë¹„ë™ê¸° ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## í•µì‹¬ ì„¤ê³„ ì›ì¹™

### 1. ë…¼ë¸”ë¡œí‚¹ ì•„í‚¤í…ì²˜
- ëª¨ë“  AI ì²˜ë¦¬ëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ
- í„°ë¯¸ë„ ì…ì¶œë ¥ ìš°ì„ ìˆœìœ„ ìµœìƒìœ„
- UI ì—…ë°ì´íŠ¸ëŠ” ë³„ë„ ìŠ¤ë ˆë“œ

### 2. ìŠ¤ë§ˆíŠ¸ íŠ¸ë¦¬ê±°ë§
- í•„ìš”í•œ ìˆœê°„ì—ë§Œ AI í™œì„±í™”
- ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²˜ë¦¬
- ì¤‘ë³µ ë¶„ì„ ë°©ì§€

## Step 1: ë¹„ë™ê¸° ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ

### utils/events.py
```python
import asyncio
from typing import Dict, Any, Callable, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum, auto
import logging
from queue import PriorityQueue
import threading

logger = logging.getLogger(__name__)


class EventType(Enum):
    """ì´ë²¤íŠ¸ íƒ€ì…"""
    COMMAND_START = auto()
    COMMAND_END = auto()
    ERROR_DETECTED = auto()
    AI_ANALYSIS_REQUEST = auto()
    AI_ANALYSIS_COMPLETE = auto()
    CONTEXT_UPDATE = auto()
    UI_UPDATE = auto()
    USER_INPUT = auto()


@dataclass(order=True)
class Event:
    """ìš°ì„ ìˆœìœ„ ì´ë²¤íŠ¸"""
    priority: int
    type: EventType = field(compare=False)
    data: Dict[str, Any] = field(compare=False)
    timestamp: datetime = field(default_factory=datetime.now, compare=False)
    callback: Optional[Callable] = field(default=None, compare=False)


class EventBus:
    """ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë²„ìŠ¤"""
    
    def __init__(self):
        self.listeners: Dict[EventType, List[Callable]] = {}
        self.event_queue = asyncio.Queue()
        self.priority_queue = PriorityQueue()
        self.running = False
        self._lock = threading.Lock()
        
    def subscribe(self, event_type: EventType, handler: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        with self._lock:
            if event_type not in self.listeners:
                self.listeners[event_type] = []
            self.listeners[event_type].append(handler)
            
    def unsubscribe(self, event_type: EventType, handler: Callable):
        """êµ¬ë… í•´ì œ"""
        with self._lock:
            if event_type in self.listeners:
                self.listeners[event_type].remove(handler)
                
    async def emit(self, event: Event):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        await self.event_queue.put(event)
        
    def emit_sync(self, event: Event):
        """ë™ê¸° ì´ë²¤íŠ¸ ë°œí–‰ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        self.priority_queue.put((event.priority, event))
        
    async def start(self):
        """ì´ë²¤íŠ¸ ë£¨í”„ ì‹œì‘"""
        self.running = True
        
        # ë¹„ë™ê¸° í ì²˜ë¦¬
        asyncio.create_task(self._process_async_queue())
        
        # ìš°ì„ ìˆœìœ„ í ì²˜ë¦¬
        asyncio.create_task(self._process_priority_queue())
        
    async def stop(self):
        """ì´ë²¤íŠ¸ ë£¨í”„ ì¤‘ì§€"""
        self.running = False
        
    async def _process_async_queue(self):
        """ë¹„ë™ê¸° í ì²˜ë¦¬"""
        while self.running:
            try:
                event = await asyncio.wait_for(
                    self.event_queue.get(), 
                    timeout=0.1
                )
                await self._handle_event(event)
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Event processing error: {e}")
                
    async def _process_priority_queue(self):
        """ìš°ì„ ìˆœìœ„ í ì²˜ë¦¬"""
        while self.running:
            try:
                if not self.priority_queue.empty():
                    _, event = self.priority_queue.get(timeout=0.1)
                    await self._handle_event(event)
                else:
                    await asyncio.sleep(0.1)
            except Exception as e:
                logger.error(f"Priority queue error: {e}")
                
    async def _handle_event(self, event: Event):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        with self._lock:
            handlers = self.listeners.get(event.type, [])
            
        for handler in handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(event)
                else:
                    handler(event)
            except Exception as e:
                logger.error(f"Handler error for {event.type}: {e}")
                
        # ì½œë°± ì‹¤í–‰
        if event.callback:
            try:
                if asyncio.iscoroutinefunction(event.callback):
                    await event.callback(event)
                else:
                    event.callback(event)
            except Exception as e:
                logger.error(f"Callback error: {e}")
```

## Step 2: ì‹¤ì‹œê°„ ë¶„ì„ íŒŒì´í”„ë¼ì¸

### ai/realtime_analyzer.py
```python
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from collections import deque

from .ollama_client import OllamaClient, AIResponse
from .context_manager import ContextManager
from ..utils.events import EventBus, Event, EventType

logger = logging.getLogger(__name__)


@dataclass
class AnalysisRequest:
    """ë¶„ì„ ìš”ì²­"""
    id: str
    context: Dict[str, Any]
    trigger_type: str
    priority: int
    timestamp: datetime
    retry_count: int = 0


class RealtimeAnalyzer:
    """ì‹¤ì‹œê°„ AI ë¶„ì„ê¸°"""
    
    def __init__(self,
                 ollama_client: OllamaClient,
                 context_manager: ContextManager,
                 event_bus: EventBus):
        self.ollama_client = ollama_client
        self.context_manager = context_manager
        self.event_bus = event_bus
        
        # ë¶„ì„ í
        self.analysis_queue: asyncio.Queue[AnalysisRequest] = asyncio.Queue()
        self.active_analyses: Dict[str, AnalysisRequest] = {}
        
        # ì„±ëŠ¥ ì œí•œ
        self.max_concurrent = 2
        self.min_interval = timedelta(seconds=2)
        self.last_analysis_time = datetime.now()
        
        # ìºì‹œ
        self.response_cache: Dict[str, AIResponse] = {}
        self.cache_size = 50
        
        # ì´ë²¤íŠ¸ êµ¬ë…
        self._subscribe_events()
        
    def _subscribe_events(self):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        self.event_bus.subscribe(
            EventType.AI_ANALYSIS_REQUEST,
            self._handle_analysis_request
        )
        
    async def start(self):
        """ë¶„ì„ê¸° ì‹œì‘"""
        # ì›Œì»¤ ì‹œì‘
        for i in range(self.max_concurrent):
            asyncio.create_task(self._analysis_worker(i))
            
        logger.info("Realtime analyzer started")
        
    async def _analysis_worker(self, worker_id: int):
        """ë¶„ì„ ì›Œì»¤"""
        logger.info(f"Analysis worker {worker_id} started")
        
        while True:
            try:
                # ìµœì†Œ ê°„ê²© ìœ ì§€
                time_since_last = datetime.now() - self.last_analysis_time
                if time_since_last < self.min_interval:
                    await asyncio.sleep(
                        (self.min_interval - time_since_last).total_seconds()
                    )
                    
                # ìš”ì²­ ê°€ì ¸ì˜¤ê¸°
                request = await self.analysis_queue.get()
                
                # ìºì‹œ í™•ì¸
                cache_key = self._get_cache_key(request.context)
                if cache_key in self.response_cache:
                    logger.info(f"Using cached response for {request.id}")
                    response = self.response_cache[cache_key]
                else:
                    # AI ë¶„ì„ ì‹¤í–‰
                    response = await self._perform_analysis(request)
                    
                    # ìºì‹œ ì €ì¥
                    self._update_cache(cache_key, response)
                    
                # ê²°ê³¼ ì´ë²¤íŠ¸ ë°œí–‰
                await self.event_bus.emit(Event(
                    priority=5,
                    type=EventType.AI_ANALYSIS_COMPLETE,
                    data={
                        "request_id": request.id,
                        "response": response,
                        "context": request.context
                    }
                ))
                
                self.last_analysis_time = datetime.now()
                
            except Exception as e:
                logger.error(f"Worker {worker_id} error: {e}")
                
                # ì¬ì‹œë„ ë¡œì§
                if request and request.retry_count < 3:
                    request.retry_count += 1
                    await self.analysis_queue.put(request)
                    
    async def _perform_analysis(self, request: AnalysisRequest) -> AIResponse:
        """AI ë¶„ì„ ìˆ˜í–‰"""
        logger.info(f"Performing analysis for {request.id}")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        filtered_context = self.context_manager.get_filtered_context()
        
        # AI í˜¸ì¶œ
        response = await self.ollama_client.analyze_context(filtered_context)
        
        return response
        
    async def _handle_analysis_request(self, event: Event):
        """ë¶„ì„ ìš”ì²­ ì²˜ë¦¬"""
        data = event.data
        
        # ì¤‘ë³µ í™•ì¸
        request_id = data.get("id", f"req_{datetime.now().timestamp()}")
        if request_id in self.active_analyses:
            logger.debug(f"Duplicate request ignored: {request_id}")
            return
            
        # ìš”ì²­ ìƒì„±
        request = AnalysisRequest(
            id=request_id,
            context=data.get("context", {}),
            trigger_type=data.get("trigger_type", "manual"),
            priority=data.get("priority", 5),
            timestamp=datetime.now()
        )
        
        # íì— ì¶”ê°€
        self.active_analyses[request_id] = request
        await self.analysis_queue.put(request)
        
    def _get_cache_key(self, context: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ìµœê·¼ ëª…ë ¹ì–´ ê¸°ë°˜ í‚¤ ìƒì„±
        commands = context.get("commands", [])[-5:]
        key_parts = [cmd.get("command", "") for cmd in commands]
        return ":".join(key_parts)
        
    def _update_cache(self, key: str, response: AIResponse):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        # í¬ê¸° ì œí•œ
        if len(self.response_cache) >= self.cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest = min(
                self.response_cache.items(),
                key=lambda x: x[1].timestamp
            )
            del self.response_cache[oldest[0]]
            
        self.response_cache[key] = response
```

## Step 3: UI ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ

### ui/realtime_updates.py
```python
from textual.message import Message
from textual.reactive import reactive
from typing import Dict, Any, List, Optional
import asyncio
from datetime import datetime

from ..utils.events import EventBus, Event, EventType
from ..ai.ollama_client import AIResponse


class AIUpdateMessage(Message):
    """AI ì—…ë°ì´íŠ¸ ë©”ì‹œì§€"""
    
    def __init__(self, response: AIResponse, context: Dict[str, Any]):
        super().__init__()
        self.response = response
        self.context = context


class RealtimeUIUpdater:
    """ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, app, event_bus: EventBus):
        self.app = app
        self.event_bus = event_bus
        
        # ì—…ë°ì´íŠ¸ í
        self.update_queue = asyncio.Queue()
        
        # ì—…ë°ì´íŠ¸ ì œí•œ
        self.max_updates_per_second = 5
        self.last_update_time = datetime.now()
        
        # ì´ë²¤íŠ¸ êµ¬ë…
        self._subscribe_events()
        
    def _subscribe_events(self):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        self.event_bus.subscribe(
            EventType.AI_ANALYSIS_COMPLETE,
            self._handle_ai_complete
        )
        self.event_bus.subscribe(
            EventType.CONTEXT_UPDATE,
            self._handle_context_update
        )
        
    async def start(self):
        """ì—…ë°ì´í„° ì‹œì‘"""
        asyncio.create_task(self._update_loop())
        
    async def _update_loop(self):
        """UI ì—…ë°ì´íŠ¸ ë£¨í”„"""
        while True:
            try:
                # ì—…ë°ì´íŠ¸ ê°€ì ¸ì˜¤ê¸°
                update = await self.update_queue.get()
                
                # ì†ë„ ì œí•œ
                await self._rate_limit()
                
                # UI ì—…ë°ì´íŠ¸
                await self._apply_update(update)
                
            except Exception as e:
                logger.error(f"UI update error: {e}")
                
    async def _rate_limit(self):
        """ì—…ë°ì´íŠ¸ ì†ë„ ì œí•œ"""
        time_since_last = (datetime.now() - self.last_update_time).total_seconds()
        min_interval = 1.0 / self.max_updates_per_second
        
        if time_since_last < min_interval:
            await asyncio.sleep(min_interval - time_since_last)
            
        self.last_update_time = datetime.now()
        
    async def _apply_update(self, update: Dict[str, Any]):
        """UI ì—…ë°ì´íŠ¸ ì ìš©"""
        update_type = update.get("type")
        
        if update_type == "ai_response":
            # AI ì‚¬ì´ë“œë°” ì—…ë°ì´íŠ¸
            self.app.call_from_thread(
                self._update_ai_sidebar,
                update["response"],
                update["context"]
            )
        elif update_type == "status":
            # ìƒíƒœ í‘œì‹œ ì—…ë°ì´íŠ¸
            self.app.call_from_thread(
                self._update_status,
                update["message"]
            )
            
    def _update_ai_sidebar(self, response: AIResponse, context: Dict[str, Any]):
        """AI ì‚¬ì´ë“œë°” ì—…ë°ì´íŠ¸"""
        sidebar = self.app.query_one("AISidebar")
        
        # ì œì•ˆì‚¬í•­ ì¶”ê°€
        for suggestion in response.suggestions:
            sidebar.add_message(suggestion, "suggestion")
            
        # ê²½ê³  ì¶”ê°€
        for warning in response.warnings:
            sidebar.add_message(warning, "warning")
            
        # ì—ëŸ¬ í•´ê²°ì±… ì¶”ê°€
        for error in response.errors:
            sidebar.add_message(error, "error")
            
    def _update_status(self, message: str):
        """ìƒíƒœ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸"""
        status_bar = self.app.query_one("#status-bar")
        status_bar.update(message)
        
    async def _handle_ai_complete(self, event: Event):
        """AI ë¶„ì„ ì™„ë£Œ ì²˜ë¦¬"""
        await self.update_queue.put({
            "type": "ai_response",
            "response": event.data["response"],
            "context": event.data["context"]
        })
        
    async def _handle_context_update(self, event: Event):
        """ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì²˜ë¦¬"""
        # ìƒíƒœ ë©”ì‹œì§€ë§Œ í‘œì‹œ
        message = event.data.get("message", "Context updated")
        await self.update_queue.put({
            "type": "status",
            "message": message
        })
```

## Step 4: í†µí•© ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ

### main_realtime.py
```python
#!/usr/bin/env python3
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from ui.app import TerminalAIApp
from terminal.manager import TerminalManager
from ai.ollama_client import OllamaClient, OllamaConfig
from ai.context_manager import ContextManager
from ai.realtime_analyzer import RealtimeAnalyzer
from ui.realtime_updates import RealtimeUIUpdater
from utils.events import EventBus, Event, EventType
from utils.config import load_config


class RealtimeTerminalAI(TerminalAIApp):
    """ì‹¤ì‹œê°„ ê¸°ëŠ¥ì´ í†µí•©ëœ í„°ë¯¸ë„ AI"""
    
    def __init__(self):
        super().__init__()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.event_bus = EventBus()
        self.terminal_manager = TerminalManager()
        self.ollama_client = OllamaClient(OllamaConfig())
        self.context_manager = ContextManager(self.terminal_manager)
        
        # ì‹¤ì‹œê°„ ì»´í¬ë„ŒíŠ¸
        self.realtime_analyzer = RealtimeAnalyzer(
            self.ollama_client,
            self.context_manager,
            self.event_bus
        )
        self.ui_updater = RealtimeUIUpdater(self, self.event_bus)
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        self._setup_event_handlers()
        
    def _setup_event_handlers(self):
        """ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        # í„°ë¯¸ë„ ì´ë²¤íŠ¸ë¥¼ ì´ë²¤íŠ¸ ë²„ìŠ¤ë¡œ ì „ë‹¬
        original_on_command_end = self.terminal_manager.on_command_end
        
        def enhanced_on_command_end(command: str, exit_code: int):
            # ì›ë˜ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
            if original_on_command_end:
                original_on_command_end(command, exit_code)
                
            # ì´ë²¤íŠ¸ ë°œí–‰
            self.event_bus.emit_sync(Event(
                priority=7,
                type=EventType.COMMAND_END,
                data={
                    "command": command,
                    "exit_code": exit_code,
                    "timestamp": datetime.now()
                }
            ))
            
            # ì—ëŸ¬ì¸ ê²½ìš° AI ë¶„ì„ ìš”ì²­
            if exit_code != 0:
                self.event_bus.emit_sync(Event(
                    priority=9,
                    type=EventType.AI_ANALYSIS_REQUEST,
                    data={
                        "trigger_type": "error",
                        "context": self.context_manager.get_filtered_context()
                    }
                ))
                
        self.terminal_manager.on_command_end = enhanced_on_command_end
        
    async def on_mount(self):
        """ì•± ë§ˆìš´íŠ¸ ì‹œ ì´ˆê¸°í™”"""
        await super().on_mount()
        
        # ì´ë²¤íŠ¸ ë²„ìŠ¤ ì‹œì‘
        await self.event_bus.start()
        
        # ì‹¤ì‹œê°„ ì»´í¬ë„ŒíŠ¸ ì‹œì‘
        await self.realtime_analyzer.start()
        await self.ui_updater.start()
        
        # ì´ˆê¸° ìƒíƒœ í™•ì¸
        if await self.ollama_client.check_health():
            self.notify("AI Assistant ready", severity="information")
        else:
            self.notify("Ollama not available", severity="warning")
            
    async def on_unmount(self):
        """ì•± ì¢…ë£Œ ì‹œ ì •ë¦¬"""
        await self.event_bus.stop()
        await super().on_unmount()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    app = RealtimeTerminalAI()
    await app.run_async()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nApplication terminated")
        sys.exit(0)
```

## Checkpoint 4: ì‹¤ì‹œê°„ í†µí•© í…ŒìŠ¤íŠ¸

### ì‹¤í–‰ ë°©ë²•
```bash
# Ollama ì„œë²„ ì‹œì‘
ollama serve

# ì‹¤ì‹œê°„ ì•± ì‹¤í–‰
python main_realtime.py
```

### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
1. **ì—ëŸ¬ íŠ¸ë¦¬ê±° í…ŒìŠ¤íŠ¸**
   ```bash
   # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼
   cat /nonexistent/file.txt
   # AIê°€ ìë™ìœ¼ë¡œ ì—ëŸ¬ ë¶„ì„
   ```

2. **Git ì‘ì—… í…ŒìŠ¤íŠ¸**
   ```bash
   git add .
   git commit -m "test"
   git push origin nonexistent-branch
   # AIê°€ Git ì—ëŸ¬ í•´ê²°ì±… ì œì‹œ
   ```

3. **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**
   ```bash
   # ì—°ì† ëª…ë ¹ì–´ ì‹¤í–‰
   for i in {1..10}; do echo "Test $i"; done
   # UIê°€ ë¸”ë¡œí‚¹ë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
   ```

### ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] í„°ë¯¸ë„ ì‘ì—… ì¤‘ UI ë°˜ì‘ì„± ìœ ì§€
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ ìë™ AI ë¶„ì„
- [ ] AI ì‘ë‹µì´ ì‚¬ì´ë“œë°”ì— ì‹¤ì‹œê°„ í‘œì‹œ
- [ ] ì—¬ëŸ¬ ë¶„ì„ì´ ë™ì‹œì— ì²˜ë¦¬ë¨
- [ ] ìºì‹œê°€ ì •ìƒ ì‘ë™í•¨
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì•ˆì •ì 
- [ ] Ctrl+Aë¡œ AI í† ê¸€ ê°€ëŠ¥

### ì˜ˆìƒ ë™ì‘
```
Terminal                          AI Assistant
--------                          ------------
$ ls /invalid                     ğŸŸ¡ Processing...
ls: /invalid: No such file        
                                 [ERROR] 12:34:56
                                 Directory not found
                                 
                                 Try:
                                 - Check spelling
                                 - Use 'ls' to see available dirs
                                 - Use 'mkdir' to create

$ git push                       ğŸŸ¡ Processing...
fatal: not a git repository      
                                 [ERROR] 12:35:01
                                 Not in a git repository
                                 
                                 Initialize with:
                                 git init
                                 git remote add origin <url>
```

## ë¬¸ì œ í•´ê²°

### 1. UI ì—…ë°ì´íŠ¸ ì§€ì—°
```python
# ì—…ë°ì´íŠ¸ ë°°ì¹˜ ì²˜ë¦¬
updates = []
while not update_queue.empty() and len(updates) < 5:
    updates.append(update_queue.get_nowait())
    
# í•œ ë²ˆì— ì ìš©
for update in updates:
    apply_update(update)
```

### 2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜
```python
# ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì •ë¦¬
def cleanup():
    for event_type, handlers in listeners.items():
        handlers.clear()
        
# ì˜¤ë˜ëœ ìºì‹œ ì£¼ê¸°ì  ì •ë¦¬
async def cleanup_cache():
    while True:
        await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤
        # ì˜¤ë˜ëœ í•­ëª© ì œê±°
```

### 3. AI ì‘ë‹µ ì†ë„
```python
# í”„ë¦¬í˜ì¹­
if likely_next_command:
    asyncio.create_task(
        prefetch_analysis(likely_next_command)
    )
    
# ë¶€ë¶„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
async for chunk in ollama_client.stream_generate(prompt):
    ui.append_chunk(chunk)
```

## ë‹¤ìŒ ë‹¨ê³„

ì‹¤ì‹œê°„ ê¸°ëŠ¥ì´ êµ¬í˜„ë˜ë©´ [07-optimization.md](07-optimization.md)ë¡œ ì§„í–‰í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.